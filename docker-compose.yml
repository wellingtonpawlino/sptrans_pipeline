version: "3.9"

services:
  # -------------------------
  # APP (API ou ETL auxiliar)
  # -------------------------
  app:
    build: .
    container_name: sptrans_app
    stdin_open: true
    tty: true
    volumes:
      - .:/app
    env_file:
      - .env
    depends_on:
      - db
    networks:
      - sptrans_net
    environment:
      - DB_HOST=db
      - DB_PORT=5432
      - DB_NAME=sptrans
      - DB_USER=postgres
      - DB_PASSWORD=postgres
    restart: unless-stopped

  # -------------------------
  # Banco de Dados Postgres
  # -------------------------
  db:
    image: postgres:15
    container_name: sptrans_db
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: sptrans
    ports:
      - "5432:5432"
    volumes:
      - pgdata:/var/lib/postgresql/data
    networks:
      - sptrans_net

  pgadmin:
    image: dpage/pgadmin4
    container_name: sptrans_pgadmin
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@admin.com
      PGADMIN_DEFAULT_PASSWORD: admin
    ports:
      - "5050:80"
    depends_on:
      - db
    volumes:
      - pgadmin_data:/var/lib/pgadmin
    networks:
      - sptrans_net

  # -------------------------
  # Airflow
  # -------------------------
  airflow-init:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    container_name: airflow_init
    depends_on:
      - db
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://postgres:postgres@db/airflow_db
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - PYTHONPATH=/opt/airflow:/opt/airflow/dags:/opt/airflow/processors:/opt/airflow/api
    entrypoint: /bin/bash
    command: -c "airflow db init"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./api:/opt/airflow/api
      - ./processors:/opt/airflow/processors
      - ./config.properties:/opt/airflow/config.properties
    networks:
      - sptrans_net

  airflow-create-user:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    container_name: airflow_create_user
    depends_on:
      - airflow-init
    entrypoint: /bin/bash
    command: >
      -c "airflow users create
      --username airflow
      --firstname Admin
      --lastname User
      --role Admin
      --email airflow@example.com
      --password airflow"
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://postgres:postgres@db/airflow_db
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - PYTHONPATH=/opt/airflow:/opt/airflow/dags:/opt/airflow/processors:/opt/airflow/api
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./api:/opt/airflow/api
      - ./processors:/opt/airflow/processors
      - ./config.properties:/opt/airflow/config.properties
    networks:
      - sptrans_net

  airflow-webserver:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    container_name: airflow_webserver
    env_file:
      - .env
    depends_on:
      - airflow-create-user
    ports:
      - "8080:8080"
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://postgres:postgres@db/airflow_db
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=True
      - PYTHONPATH=/opt/airflow:/opt/airflow/dags:/opt/airflow/processors:/opt/airflow/api
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./api:/opt/airflow/api
      - ./processors:/opt/airflow/processors
      - ./config.properties:/opt/airflow/config.properties
      - ./data:/opt/airflow/data
      - /var/run/docker.sock:/var/run/docker.sock
    command: webserver
    networks:
      - sptrans_net

  airflow-scheduler:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    container_name: airflow_scheduler
    depends_on:
      - airflow-webserver
    env_file:
      - .env
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://postgres:postgres@db/airflow_db
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - PYTHONPATH=/opt/airflow:/opt/airflow/dags:/opt/airflow/processors:/opt/airflow/api
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
      - ./api:/opt/airflow/api
      - ./processors:/opt/airflow/processors
      - ./config.properties:/opt/airflow/config.properties
      - ./data:/opt/airflow/data
      - /var/run/docker.sock:/var/run/docker.sock
    command: scheduler
    networks:
      - sptrans_net

  # -------------------------
  # MinIO
  # -------------------------
  minio:
    image: minio/minio
    container_name: minio
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    command: server /data --console-address ":9001"
    volumes:
      - minio_data:/data
    networks:
      - sptrans_net

  # -------------------------
  # Metabase
  # -------------------------
  metabase:
    image: metabase/metabase:latest
    container_name: sptrans_metabase
    ports:
      - "3000:3000"
    environment:
      MB_DB_TYPE: postgres
      MB_DB_DBNAME: sptrans
      MB_DB_PORT: 5432
      MB_DB_USER: postgres
      MB_DB_PASS: postgres
      MB_DB_HOST: db
    depends_on:
      - db
    networks:
      - sptrans_net

  # -------------------------
  # Jupyter Notebook
  # -------------------------
  jupyter:
    build:
      context: .
      dockerfile: Dockerfile.jupyter
    container_name: sptrans_jupyter
    ports:
      - "8889:8888"
    volumes:
      - ./notebooks:/home/jovyan/work
    command: start-notebook.sh --NotebookApp.token='' --NotebookApp.password='' --NotebookApp.default_url=/lab
    networks:
      - sptrans_net
    depends_on:
      - minio
      - spark

  # -------------------------
  # Spark Master
  # -------------------------
  spark:
    build:
      context: .
      dockerfile: Dockerfile.spark
    container_name: sptrans_spark_master
    hostname: spark
    environment:
      - SPARK_HOME=/opt/spark
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.master.Master", "--host", "spark", "--port", "7077", "--webui-port", "8080"]
    ports:
      - "7077:7077"
      - "8081:8080"
    networks:
      - sptrans_net
    volumes:
      - ./processors:/opt/airflow/processors
      - ./data:/opt/airflow/data
      - spark_data:/opt/spark/data

  # -------------------------
  # Spark Worker
  # -------------------------
  spark-worker:
    build:
      context: .
      dockerfile: Dockerfile.spark
    container_name: sptrans_spark_worker
    environment:
      - SPARK_HOME=/opt/spark
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.worker.Worker", "spark://spark:7077"]
    depends_on:
      - spark
    networks:
      - sptrans_net
    volumes:
      - ./processors:/opt/airflow/processors
      - ./data:/opt/airflow/data
      - spark_data:/opt/spark/data

volumes:
  pgdata:
  pgadmin_data:
  minio_data:
  spark_data:

networks:
  sptrans_net: